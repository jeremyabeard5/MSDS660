---
title: "MSDS 660 Week 6 Assignment"
author: "Jeremy Beard"
date: '2022-08-10'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Introduction 

This week's assignment will focus on building different types of regression models. We will be working with a "churn" dataset which contains data describing customer churn within a corporation. It was supplied by the instructor of this MSDS 660 course. It is a mostly clean dataset with few null values. Train and Test datasets will be created from this dataset.

After creating the model, we will work to improve the model through VIF analysis for collinearity, analyzing correlation coefficients, and running StepAIC analyses. 

Finally, after the model has been refined, predictions will be made from the train and test data created before, using the refined regression model.

First, we'll load the libraries necessary, set the seed, and load in the data:
```{r}

# load libraries
library(data.table)
library(dplyr)
library(car)
library(caret)
library(caTools)
library(pROC)
library(MASS)
library(ggcorrplot)

# set the seed
set.seed(1)

#load data as datatable 
dt <- read.csv("C:\\Users\\jerem\\OneDrive\\Documents\\School\\_REGIS\\2022-05_Summer\\MSDS660\\Week6\\churn.csv", sep = ",")
dt <- as.data.table(dt)

 
```

Next, we'll compute some summaries, remove the ID column as it doesn't provide useful statistical information, and display the unique entries in each column, for later factoring:

```{r}

str(dt)
summary(dt)

```

It looks like there are a LOT of categorical and nominal data. I should create factors from these columns so they are more easily able to be worked with.

```{r}

dt <- dt[, !"customerID"]
str(dt)
summary(dt)

unique(dt$gender)
unique(dt$SeniorCitizen)
unique(dt$Partner)
unique(dt$Dependents)
#unique(dt$tenure)
unique(dt$PhoneService)
unique(dt$MultipleLines)
unique(dt$InternetService)
unique(dt$OnlineSecurity)
unique(dt$OnlineBackup)
unique(dt$DeviceProtection)
unique(dt$TechSupport)
unique(dt$StreamingTV)
unique(dt$StreamingMovies)
unique(dt$Contract)
unique(dt$PaperlessBilling)
unique(dt$PaymentMethod)
#unique(dt$MonthlyCharges)
#unique(dt$TotalCharges)
unique(dt$Churn)

```

Now we're ready to change all 'char' columns to be factors, based on the unique entries from each column:

```{r}
# run tests

#Factor class and relable as benign or malignant
dt$Churn <- factor(dt$Churn, labels = c('No', 'Yes'))
dt$gender <- factor(dt$gender, labels = c('Male', 'Female'))
dt$Partner <- factor(dt$Partner, labels = c('No', 'Yes'))
dt$Dependents <- factor(dt$Dependents, labels = c('No', 'Yes'))
dt$PhoneService <- factor(dt$PhoneService, labels = c('No', 'Yes'))
head(dt$MultipleLines)
dt$MultipleLines <- factor(dt$MultipleLines, labels = c('No', 'Yes', 'No phone service'))
dt$InternetService <- factor(dt$InternetService, labels = c('No', 'DSL', 'Fiber optic'))
dt$OnlineSecurity <- factor(dt$OnlineSecurity, labels = c('No', 'Yes', 'No internet service'))
dt$OnlineBackup <- factor(dt$OnlineBackup, labels = c('No', 'Yes', 'No internet service'))
dt$DeviceProtection <- factor(dt$DeviceProtection, labels = c('No', 'Yes', 'No internet service'))
dt$TechSupport <- factor(dt$TechSupport, labels = c('No', 'Yes', 'No internet service'))
dt$StreamingTV <- factor(dt$StreamingTV, labels = c('No', 'Yes', 'No internet service'))
dt$StreamingMovies <- factor(dt$StreamingMovies, labels = c('No', 'Yes', 'No internet service'))
dt$Contract <- factor(dt$Contract, labels = c('Month-to-month', 'One year', 'Two year'))
dt$PaperlessBilling <- factor(dt$PaperlessBilling, labels = c('No', 'Yes'))
dt$PaymentMethod <- factor(dt$PaymentMethod, labels = c('Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'))

```

Next we'll find and remove any and all null values:

```{r}

str(dt)
summary(dt)

#remove NAs
#first, we'll just list how many NA's are present
which(is.na(dt$gender))
which(is.na(dt$SeniorCitizen))
which(is.na(dt$Partner))
which(is.na(dt$Dependents))
which(is.na(dt$tenure))
which(is.na(dt$PhoneService))
which(is.na(dt$MultipleLines))
which(is.na(dt$InternetService))
which(is.na(dt$OnlineSecurity))
which(is.na(dt$OnlineBackup))
which(is.na(dt$DeviceProtection))
which(is.na(dt$TechSupport))
which(is.na(dt$StreamingTV))
which(is.na(dt$StreamingMovies))
which(is.na(dt$Contract))
which(is.na(dt$PaperlessBilling))
which(is.na(dt$PaymentMethod))
which(is.na(dt$MonthlyCharges))
which(is.na(dt$TotalCharges))
which(is.na(dt$Churn))

#now we'll remove the NA's and check to make sure they're gone
dt <- dt[complete.cases(dt), ]
which(is.na(dt$customerID))
which(is.na(dt$gender))
which(is.na(dt$SeniorCitizen))
which(is.na(dt$Partner))
which(is.na(dt$Dependents))
which(is.na(dt$tenure))
which(is.na(dt$PhoneService))
which(is.na(dt$MultipleLines))
which(is.na(dt$InternetService))
which(is.na(dt$OnlineSecurity))
which(is.na(dt$OnlineBackup))
which(is.na(dt$DeviceProtection))
which(is.na(dt$TechSupport))
which(is.na(dt$StreamingTV))
which(is.na(dt$StreamingMovies))
which(is.na(dt$Contract))
which(is.na(dt$PaperlessBilling))
which(is.na(dt$PaymentMethod))
which(is.na(dt$MonthlyCharges))
which(is.na(dt$TotalCharges))
which(is.na(dt$Churn))
#NA's have been removed from the dataset!


```



#### Methods 

Now we'll split the data up into train and test data and create a multi linear binomial logistic regression mode. We will then try to improve the model to a level which can be called significant. We will check for collinearity, run StepAIC analyses, plot correlation plots. Our null hypothesis is that there is no relationship between any of the data and the Churn column. The alternate hypothesis is that there is indeed a significant correlation between the data and the Churn column. The methods used in the assignment will hope to disprove the null hypothesis and find a correlation in the data. Let's begin:

```{r}


#Now time to split the data into a train and test set

#split the data into a train and test set
samp <- sample.split(dt$Churn, SplitRatio = 0.8)
train <- subset(dt, samp == TRUE)
test <- subset(dt, samp == FALSE)

# Create a multi linear binomial logisitc regression
model <- glm(Churn ~ ., data = train, family = "binomial")

# Look at the model summary
summary(model)

# Check for colinearity
###########vif(model)
#hmmmm, getting the error: "Error in vif.default(model) : there are aliased coefficients in the model"
#it means 2+ variables are very closely related
#let's plot a correlation matrix to see which ones

model.matrix(~0+., data=dt) %>%
  cor(use="pairwise.complete.obs") %>%
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

# I will remove features which have a correlation of 1.
# this is InternetServiceFiber optic -- OnlineSecurity Yes
#   InternetServiceFiber optic -- Online Backup Yes
#   InternetServiceFiber optic -- DeviceProtection Yees
#   InternetServiceFiber optic -- TechSupport Yes
#   InternetServiceFiber optic -- StreamingTV Yes
#   InternetServiceFiber optic -- StreamingMovies Yes

# So looks like I just need to keep one of these 7 features. I will keep InternetService

dt <- dt[, !"OnlineSecurity"]
dt <- dt[, !"OnlineBackup"]
dt <- dt[, !"DeviceProtection"]
dt <- dt[, !"TechSupport"]
dt <- dt[, !"StreamingTV"]
dt <- dt[, !"StreamingMovies"]

samp <- sample.split(dt$Churn, SplitRatio = 0.8)
train <- subset(dt, samp == TRUE)
test <- subset(dt, samp == FALSE)

# Create a multi linear binomial logisitc regression
model <- glm(Churn ~ ., data = train, family = "binomial")

# Look at the model summary
summary(model)

# Check for colinearity
############vif(model)

#Hmm, still getting the error

str(dt)
model.matrix(~0+., data=dt) %>%
  cor(use="pairwise.complete.obs") %>%
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

# Looks like I should remove the -1 correlations too
# This is: 
# PhoneService Yes -- MultipleLines Yes
# genderMale -- genderFemale

# Try #3
dt <- dt[, !"gender"]
dt <- dt[, !"MultipleLines"]

samp <- sample.split(dt$Churn, SplitRatio = 0.8)
train <- subset(dt, samp == TRUE)
test <- subset(dt, samp == FALSE)

# Create a multi linear binomial logisitc regression
model <- glm(Churn ~ ., data = train, family = "binomial")

# Look at the model summary
summary(model)

# Check for colinearity
vif(model)
# Whew okay, looks good


# Looks like tenure, InternetService, MonthlyCharges, and TotalCharges have a GVIF over 5. 
# Let's see the correlation matrix again
model.matrix(~0+., data=dt) %>%
  cor(use="pairwise.complete.obs") %>%
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

#Yeah, the 3 variables above have correlations above |0.75|. 
# AND PartnerNo has a correlation of 1 to PartnerYes. I will remove them all
dt <- dt[, !"Partner"]
dt <- dt[, !"InternetService"]
dt <- dt[, !"MonthlyCharges"]
dt <- dt[, !"TotalCharges"]

samp <- sample.split(dt$Churn, SplitRatio = 0.8)
train <- subset(dt, samp == TRUE)
test <- subset(dt, samp == FALSE)

# Create a multi linear binomial logisitc regression
model <- glm(Churn ~ ., data = train, family = "binomial")

# Look at the model summary
summary(model)

# Check for colinearity
vif(model)
# Whew okay, looks even better. No GVIF above 5 (should I be using GVIF^(1/(2*Df))???)

# Perform stepAIC to remove high p-values
stepAIC(model, direction = 'both')

model <- glm(formula = Churn ~ SeniorCitizen + Dependents + tenure + PhoneService + Contract + PaperlessBilling + PaymentMethod, family = "binomial", data = train)

#Check model summary again
summary(model)
```


#### Results 


The results of these tests turned out to be pretty straightforward. I wanted to split some of the code up between the Methods section and the Results section so some of what is mentioned below may be contained in the Methods section, but the results of the tests showed a similar result between the train and the test data, both of which had high accuracy scores of ~78%. This shows that the model had a good fit. All final confusion matrices had Mcnemar's Test P-Value of under <0.05 which shows that the null hypothesis could be rejected and there actually was a significant correlation between the data and the Churn information. This is to be expected as there should be some relationship between the data of an employee and if they will "churn" or not, it cannot be completely random.



```{r}
# predict on the train data            
trainpreds <- predict(model, type = 'response', train)

# Round prediction values at 0.5 cutoff factor and change lables
trainp <- factor(trainpreds >= 0.5, labels = c('No', 'Yes'))

# Build a confusion matrix and see results
trainCM <- confusionMatrix(train$Churn, trainp)
trainCM

# predict on the test data            
testpreds <- predict(model, type = 'response', test)

# Round prediction values at 0.5 cutoff factor and change labels
testp <- factor(testpreds >= 0.5, labels = c('No', 'Yes'))

# Build a confusion matrix and see results
testCM <- confusionMatrix(test$Churn, testp)
testCM

# Create a Roc curve and and view ROC results for the Train data
train_roc_curve <- roc(train$Churn, trainpreds)
train_roc_curve
plot(train_roc_curve)
train_rocc <- coords(roc=train_roc_curve, x = 'best', best.method = 'closest.topleft')
train_rocc

# Create a Roc curve and view results for the Test data
test_roc_curve <- roc(test$Churn, testpreds)
test_roc_curve
plot(test_roc_curve)
test_rocc <- coords(roc=test_roc_curve, x = 'best', best.method = 'closest.topleft')
test_rocc

# predict on the train data using the ROC cutoff            
# Round prediction values at threshold level and change labels
trainrocp <- factor(trainpreds >= as.numeric(train_rocc[1]), labels = c('No', 'Yes'))

# Build a confusion matrix to see results
trainROCCM <- confusionMatrix(train$Churn, trainrocp)
trainROCCM

# predict on the test data using the ROC cutoff         
#  Round prediction values at threshold level and change labels
testp <- factor(testpreds >= as.numeric(test_rocc[1]), labels = c('No', 'Yes'))

# Buld a confusion matrix to see results
testROCCM <- confusionMatrix(test$Churn, testp)
testROCCM

#View all the Confusion matrices
trainCM
trainROCCM

testCM
testROCCM






```

#### Conclusion 

From the results of the tests, we can confidently conclude that there was a relationship between customer churn, and the significant non-collinear variables which were collected of the customer, such as tensure, PaymentMethod, and SeniorCitizen. This means that the regression model can be used to predict, albeit only ~75% of the time correctly, if a customer will "churn" or not, at the 95% confidence level. The testing was mostly straightforward and the big job with this dataset was removing the collinear features. There were many of them! In the future, one way to improve this effort is to find more datafields to collect from the customers that are not collinear. I found a lot of collinear features in this dataset and maybe that is normal behavior of an organic dataset. However, maybe it is abnormal and more significant datafields should be sought from the customers. 

Thank you!

Jeremy Beard